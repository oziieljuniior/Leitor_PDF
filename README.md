# ğŸ“š Chatbot Local para PDFs com IA (Offline)

Este projeto implementa um **chatbot inteligente offline** que:

- LÃª e processa arquivos PDF extensos.
- Divide automaticamente o conteÃºdo em partes otimizadas.
- Gera embeddings e indexaÃ§Ã£o vetorial com FAISS.
- Responde perguntas sobre o conteÃºdo dos PDFs utilizando modelos de linguagem locais via `llama-cpp`.

---

## âœ… Funcionalidades

- ğŸ“– Leitura e extraÃ§Ã£o de texto de PDFs grandes.
- ğŸ” DivisÃ£o automÃ¡tica em partes menores para economia de memÃ³ria RAM.
- ğŸ§  Embeddings com `sentence-transformers`.
- ğŸ” Busca distribuÃ­da com FAISS.
- ğŸ—£ï¸ GeraÃ§Ã£o de respostas com LLM local, como `Phi-2`.

---

## ğŸ›  Requisitos

- Python 3.8 ou superior
- RAM mÃ­nima: **4 GB livres**
- InstalaÃ§Ã£o das dependÃªncias:

```bash
pip install -r requirements.txt
````

---

## ğŸ“ Estrutura de DiretÃ³rios

Com base no projeto atual:

```
.
â”œâ”€â”€ documentos/
â”‚   â””â”€â”€ SAGE_ManCfg_Anx17_61850 1.pdf    # PDF original
â”œâ”€â”€ models/
â”‚   â””â”€â”€ phi-2.Q4_K_M.gguf                # Modelo LLM local
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ source.pdf                       # PDF convertido ou de trabalho
â”œâ”€â”€ pyproject/
â”‚   â”œâ”€â”€ partes_pdf/
â”‚   â”‚   â”œâ”€â”€ parte_1.pdf â†’ parte_6.pdf     # PDFs divididos
â”‚   â”‚   â”œâ”€â”€ parte_1.faiss â†’ parte_6.faiss # Ãndices vetoriais FAISS
â”‚   â”‚   â”œâ”€â”€ parte_1_chunks.pkl â†’ parte_6_chunks.pkl # Trechos de texto
â”‚   â””â”€â”€ source.ipynb                     # Notebook de controle
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ saida.txt                            # Estrutura exportada
```

---

## â–¶ï¸ Como Usar

### 1. Processar o PDF:

```bash
python processar_pdf.py
```

Isso irÃ¡:

* Dividir o PDF em partes.
* Criar os embeddings de texto.
* Salvar Ã­ndices `.faiss` e trechos `.pkl`.

### 2. Realizar perguntas:

```bash
python responder_pergunta.py
```

Digite uma pergunta sobre o conteÃºdo do PDF e o chatbot retornarÃ¡ a resposta com base nos trechos relevantes.

---

## ğŸ“š Modelos Recomendados

| Nome       | RAM sugerida | Link                                                                                               |
| ---------- | ------------ | -------------------------------------------------------------------------------------------------- |
| Phi-2      | 3â€“4 GB       | [https://huggingface.co/TheBloke/Phi-2-GGUF](https://huggingface.co/TheBloke/Phi-2-GGUF)           |
| TinyLLaMA  | 2 GB         | [https://huggingface.co/TheBloke/TinyLlama](https://huggingface.co/TheBloke/TinyLlama)             |
| Mistral 7B | 6â€“8 GB       | [https://huggingface.co/TheBloke/Mistral-7B-GGUF](https://huggingface.co/TheBloke/Mistral-7B-GGUF) |

Coloque os arquivos `.gguf` em `./models/`.

---

## ğŸ’¡ Melhorias Futuras

* [ ] ğŸ” Ranking global entre partes para priorizaÃ§Ã£o de contexto.
* [ ] ğŸ§  Fine-tuning local com PDFs especÃ­ficos.
* [ ] ğŸ’¬ Interface com Gradio ou Streamlit.
* [ ] ğŸ—‚ Cache de embeddings por documento.
* [ ] ğŸ“¦ Empacotamento via `PyInstaller` ou `Docker`.

---

## ğŸ” LicenÃ§a

Uso educacional e pessoal. Consulte as licenÃ§as dos modelos utilizados.

---

## ğŸ™Œ Autor

Desenvolvido por **Oziel Ramos** com suporte do ChatGPT-4.

